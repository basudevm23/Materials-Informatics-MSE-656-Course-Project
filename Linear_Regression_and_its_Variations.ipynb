{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkoIFoCIc-Np",
        "outputId": "0f7c9716-e181-4417-da7c-aecbaad27c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (3.8.4)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\chand.chandan\\anaconda3\\envs\\mlenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas matplotlib scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "953JsPCzQGZJ"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# # -------------------- Load & Clean --------------------\n",
        "# df = pd.read_csv(\"processed_features.csv\")\n",
        "# df = df.fillna(df.mean(numeric_only=True))  # column-mean imputation\n",
        "\n",
        "# X = df.drop([\"Mu\"], axis=1).to_numpy(dtype=np.float64)\n",
        "# y = df[\"Mu\"].to_numpy(dtype=np.float64)\n",
        "# y = np.nan_to_num(y, nan=np.nanmean(y)).astype(np.float64)\n",
        "\n",
        "# # -------------------- Split then scale base X --------------------\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y, test_size=0.2, random_state=42\n",
        "# )\n",
        "# base_scaler = StandardScaler()\n",
        "# X_train = base_scaler.fit_transform(X_train).astype(np.float64)\n",
        "# X_test  = base_scaler.transform(X_test).astype(np.float64)\n",
        "\n",
        "# # -------------------- Safe GD helper --------------------\n",
        "# def gd_fit(\n",
        "#     Xm, ym, lr=1e-3, iters=8000, l2=1e-4, clip=1.0\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Batch gradient descent minimizing (1/m)||XÎ¸ - y||^2 + l2||Î¸_{no-bias}||^2.\n",
        "#     - l2 not applied to bias term.\n",
        "#     - gradient clipping by global norm.\n",
        "#     \"\"\"\n",
        "#     m, d = Xm.shape\n",
        "#     theta = np.zeros(d, dtype=np.float64)\n",
        "#     for _ in range(iters):\n",
        "#         err = Xm @ theta - ym  # shape (m,)\n",
        "#         if not np.isfinite(err).all():\n",
        "#             return None  # signal divergence\n",
        "\n",
        "#         # grad of MSE part: (2/m) X^T (XÎ¸ - y)\n",
        "#         grad = (2.0 / m) * (Xm.T @ err)\n",
        "\n",
        "#         # L2 (exclude bias at index 0)\n",
        "#         grad[1:] += 2.0 * l2 * theta[1:]\n",
        "\n",
        "#         # gradient clipping\n",
        "#         gnorm = np.linalg.norm(grad)\n",
        "#         if not np.isfinite(gnorm):\n",
        "#             return None\n",
        "#         if gnorm > clip:\n",
        "#             grad = grad * (clip / gnorm)\n",
        "\n",
        "#         theta -= lr * grad\n",
        "\n",
        "#         if not np.isfinite(theta).all():\n",
        "#             return None\n",
        "#     return theta\n",
        "\n",
        "# def standardize_poly_keep_bias(X_train_poly, X_test_poly):\n",
        "#     \"\"\"\n",
        "#     Standardize polynomial features EXCEPT the bias column (col 0).\n",
        "#     Returns standardized versions with the bias column intact.\n",
        "#     \"\"\"\n",
        "#     Xtr = X_train_poly.copy().astype(np.float64)\n",
        "#     Xte = X_test_poly.copy().astype(np.float64)\n",
        "\n",
        "#     if Xtr.shape[1] > 1:\n",
        "#         scaler = StandardScaler()\n",
        "#         Xtr[:, 1:] = scaler.fit_transform(Xtr[:, 1:])\n",
        "#         Xte[:, 1:] = scaler.transform(Xte[:, 1:])\n",
        "\n",
        "#     # Ensure exact bias column of ones\n",
        "#     Xtr[:, 0] = 1.0\n",
        "#     Xte[:, 0] = 1.0\n",
        "#     return Xtr, Xte\n",
        "\n",
        "# # -------------------- Train/eval degrees 0..7 with safeguards --------------------\n",
        "# degree_rmse = []\n",
        "# print(\"\\n---- Degree (0â€“7) vs RMSE (Stable Gradient Descent) ----\")\n",
        "\n",
        "# lrs_to_try = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4]\n",
        "\n",
        "# for deg in range(0, 8):\n",
        "#     poly = PolynomialFeatures(degree=deg, include_bias=True)\n",
        "#     X_train_poly = poly.fit_transform(X_train)\n",
        "#     X_test_poly  = poly.transform(X_test)\n",
        "\n",
        "#     # Re-standardize expanded features (except bias) to avoid huge magnitudes\n",
        "#     X_train_poly, X_test_poly = standardize_poly_keep_bias(X_train_poly, X_test_poly)\n",
        "\n",
        "#     theta = None\n",
        "#     for lr in lrs_to_try:\n",
        "#         theta = gd_fit(\n",
        "#             X_train_poly, y_train,\n",
        "#             lr=lr, iters=10000 if deg >= 4 else 8000,\n",
        "#             l2=1e-4 if deg <= 3 else 5e-4,   # a bit more L2 for higher degrees\n",
        "#             clip=1.0\n",
        "#         )\n",
        "#         if theta is not None:\n",
        "#             break  # succeeded at this lr\n",
        "\n",
        "#     if theta is None:\n",
        "#         print(f\"Degree {deg:2d} | RMSE = NaN (diverged even with small LR)\")\n",
        "#         degree_rmse.append([deg, np.nan])\n",
        "#         continue\n",
        "\n",
        "#     y_pred = X_test_poly @ theta\n",
        "#     y_pred = np.nan_to_num(y_pred, posinf=1e308, neginf=-1e308)\n",
        "\n",
        "#     rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "#     degree_rmse.append([deg, rmse])\n",
        "#     print(f\"Degree {deg:2d} | RMSE = {rmse:.6f}\")\n",
        "\n",
        "# # -------------------- Save RMSE table --------------------\n",
        "# rmse_df = pd.DataFrame(degree_rmse, columns=[\"Degree\", \"RMSE\"])\n",
        "# rmse_df.to_csv(\"degree_rmse_gd.csv\", index=False)\n",
        "# print(\"\\nðŸ“ Saved: degree_rmse_gd.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A42T-mIsoq2",
        "outputId": "5a913400-b77b-45e2-f243-21e09c3500bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---- Degree vs RMSE (Gradient Descent) ----\n",
            "Degree  0 | RMSE_train = 22.015441 | RMSE_test = 15.575324 | lr=0.01\n",
            "Degree  1 | RMSE_train = 20.096732 | RMSE_test = 14.864647 | lr=0.01\n",
            "Degree  2 | RMSE_train = 14.277297 | RMSE_test = 22.699547 | lr=0.01\n",
            "Degree  3 | RMSE_train = 6.991245 | RMSE_test = 51.622014 | lr=0.01\n",
            "Degree  4 | RMSE_train = 5.358354 | RMSE_test = 46.740635 | lr=0.01\n",
            "Degree  5 | RMSE_train = 6.865224 | RMSE_test = 34.911918 | lr=0.01\n",
            "Saved: gd_degree_rmse.csv\n",
            "\n",
            "Best (plain GD) degree = 1 with Test RMSE = 14.864647\n",
            "\n",
            "GD Train RMSE every 50 iterations (best degree):\n",
            "iter   50: 22.685330\n",
            "iter  100: 22.408780\n",
            "iter  150: 22.153319\n",
            "iter  200: 21.917958\n",
            "iter  250: 21.701600\n",
            "iter  300: 21.503096\n",
            "iter  350: 21.321305\n",
            "iter  400: 21.155153\n",
            "iter  450: 21.003676\n",
            "iter  500: 20.866043\n",
            "iter  550: 20.741558\n",
            "iter  600: 20.629644\n",
            "iter  650: 20.529816\n",
            "iter  700: 20.441653\n",
            "iter  750: 20.364767\n",
            "iter  800: 20.298775\n",
            "iter  850: 20.243276\n",
            "iter  900: 20.197830\n",
            "iter  950: 20.161927\n",
            "iter 1000: 20.134950\n",
            "iter 1050: 20.116119\n",
            "iter 1100: 20.105164\n",
            "iter 1150: 20.100761\n",
            "iter 1200: 20.098807\n",
            "iter 1250: 20.097847\n",
            "iter 1300: 20.097344\n",
            "iter 1350: 20.097072\n",
            "iter 1400: 20.096923\n",
            "iter 1450: 20.096839\n",
            "iter 1500: 20.096793\n",
            "iter 1550: 20.096766\n",
            "iter 1600: 20.096752\n",
            "iter 1650: 20.096743\n",
            "iter 1700: 20.096739\n",
            "iter 1750: 20.096736\n",
            "iter 1800: 20.096734\n",
            "iter 1850: 20.096734\n",
            "iter 1900: 20.096733\n",
            "iter 1950: 20.096733\n",
            "iter 2000: 20.096733\n",
            "iter 2050: 20.096733\n",
            "iter 2100: 20.096732\n",
            "iter 2150: 20.096732\n",
            "iter 2200: 20.096732\n",
            "iter 2250: 20.096732\n",
            "iter 2300: 20.096732\n",
            "iter 2350: 20.096732\n",
            "iter 2400: 20.096732\n",
            "iter 2450: 20.096732\n",
            "iter 2500: 20.096732\n",
            "iter 2550: 20.096732\n",
            "iter 2600: 20.096732\n",
            "iter 2650: 20.096732\n",
            "iter 2700: 20.096732\n",
            "iter 2750: 20.096732\n",
            "iter 2800: 20.096732\n",
            "iter 2850: 20.096732\n",
            "iter 2900: 20.096732\n",
            "iter 2950: 20.096732\n",
            "iter 3000: 20.096732\n",
            "iter 3050: 20.096732\n",
            "iter 3100: 20.096732\n",
            "iter 3150: 20.096732\n",
            "iter 3200: 20.096732\n",
            "iter 3250: 20.096732\n",
            "iter 3300: 20.096732\n",
            "iter 3350: 20.096732\n",
            "iter 3400: 20.096732\n",
            "iter 3450: 20.096732\n",
            "iter 3500: 20.096732\n",
            "iter 3550: 20.096732\n",
            "iter 3600: 20.096732\n",
            "iter 3650: 20.096732\n",
            "iter 3700: 20.096732\n",
            "iter 3750: 20.096732\n",
            "iter 3800: 20.096732\n",
            "iter 3850: 20.096732\n",
            "iter 3900: 20.096732\n",
            "iter 3950: 20.096732\n",
            "iter 4000: 20.096732\n",
            "iter 4050: 20.096732\n",
            "iter 4100: 20.096732\n",
            "iter 4150: 20.096732\n",
            "iter 4200: 20.096732\n",
            "iter 4250: 20.096732\n",
            "iter 4300: 20.096732\n",
            "iter 4350: 20.096732\n",
            "iter 4400: 20.096732\n",
            "iter 4450: 20.096732\n",
            "iter 4500: 20.096732\n",
            "iter 4550: 20.096732\n",
            "iter 4600: 20.096732\n",
            "iter 4650: 20.096732\n",
            "iter 4700: 20.096732\n",
            "iter 4750: 20.096732\n",
            "iter 4800: 20.096732\n",
            "iter 4850: 20.096732\n",
            "iter 4900: 20.096732\n",
            "iter 4950: 20.096732\n",
            "iter 5000: 20.096732\n",
            "iter 5050: 20.096732\n",
            "iter 5100: 20.096732\n",
            "iter 5150: 20.096732\n",
            "iter 5200: 20.096732\n",
            "iter 5250: 20.096732\n",
            "iter 5300: 20.096732\n",
            "iter 5350: 20.096732\n",
            "iter 5400: 20.096732\n",
            "iter 5450: 20.096732\n",
            "iter 5500: 20.096732\n",
            "iter 5550: 20.096732\n",
            "iter 5600: 20.096732\n",
            "iter 5650: 20.096732\n",
            "iter 5700: 20.096732\n",
            "iter 5750: 20.096732\n",
            "iter 5800: 20.096732\n",
            "iter 5850: 20.096732\n",
            "iter 5900: 20.096732\n",
            "iter 5950: 20.096732\n",
            "iter 6000: 20.096732\n",
            "iter 6050: 20.096732\n",
            "iter 6100: 20.096732\n",
            "iter 6150: 20.096732\n",
            "iter 6200: 20.096732\n",
            "iter 6250: 20.096732\n",
            "iter 6300: 20.096732\n",
            "iter 6350: 20.096732\n",
            "iter 6400: 20.096732\n",
            "iter 6450: 20.096732\n",
            "iter 6500: 20.096732\n",
            "iter 6550: 20.096732\n",
            "iter 6600: 20.096732\n",
            "iter 6650: 20.096732\n",
            "iter 6700: 20.096732\n",
            "iter 6750: 20.096732\n",
            "iter 6800: 20.096732\n",
            "iter 6850: 20.096732\n",
            "iter 6900: 20.096732\n",
            "iter 6950: 20.096732\n",
            "iter 7000: 20.096732\n",
            "iter 7050: 20.096732\n",
            "iter 7100: 20.096732\n",
            "iter 7150: 20.096732\n",
            "iter 7200: 20.096732\n",
            "iter 7250: 20.096732\n",
            "iter 7300: 20.096732\n",
            "iter 7350: 20.096732\n",
            "iter 7400: 20.096732\n",
            "iter 7450: 20.096732\n",
            "iter 7500: 20.096732\n",
            "iter 7550: 20.096732\n",
            "iter 7600: 20.096732\n",
            "iter 7650: 20.096732\n",
            "iter 7700: 20.096732\n",
            "iter 7750: 20.096732\n",
            "iter 7800: 20.096732\n",
            "iter 7850: 20.096732\n",
            "iter 7900: 20.096732\n",
            "iter 7950: 20.096732\n",
            "iter 8000: 20.096732\n",
            "Saved: gd_contour_degree_1.png\n",
            "\n",
            "---- Degree/Alpha vs RMSE (Ridge via GD) ----\n",
            "Degree  0 | best alpha=1e-05 | RMSE_test=15.575324\n",
            "Degree  1 | best alpha=0.316228 | RMSE_test=14.614969\n",
            "Degree  2 | best alpha=1 | RMSE_test=13.443738\n",
            "Degree  3 | best alpha=3.16228 | RMSE_test=12.895383\n",
            "Degree  4 | best alpha=10 | RMSE_test=13.495440\n",
            "Degree  5 | best alpha=10 | RMSE_test=16.163420\n",
            "Saved: ridge_degree_alpha_rmse.csv\n",
            "\n",
            "Best Ridge: degree=3, alpha=3.16228, Test RMSE=12.895383\n",
            "\n",
            "Ridge-GD Train RMSE every 50 iterations (best degree/alpha):\n",
            "iter   50: 21.695027\n",
            "iter  100: 20.905394\n",
            "iter  150: 20.237185\n",
            "iter  200: 19.632421\n",
            "iter  250: 19.080091\n",
            "iter  300: 18.575047\n",
            "iter  350: 18.115908\n",
            "iter  400: 17.705224\n",
            "iter  450: 17.348330\n",
            "iter  500: 17.049957\n",
            "iter  550: 16.810156\n",
            "iter  600: 16.623239\n",
            "iter  650: 16.480419\n",
            "iter  700: 16.372932\n",
            "iter  750: 16.293636\n",
            "iter  800: 16.237313\n",
            "iter  850: 16.200412\n",
            "iter  900: 16.181010\n",
            "iter  950: 16.177740\n",
            "iter 1000: 16.176930\n",
            "iter 1050: 16.176692\n",
            "iter 1100: 16.176622\n",
            "iter 1150: 16.176602\n",
            "iter 1200: 16.176596\n",
            "iter 1250: 16.176594\n",
            "iter 1300: 16.176593\n",
            "iter 1350: 16.176593\n",
            "iter 1400: 16.176593\n",
            "iter 1450: 16.176593\n",
            "iter 1500: 16.176593\n",
            "iter 1550: 16.176593\n",
            "iter 1600: 16.176593\n",
            "iter 1650: 16.176593\n",
            "iter 1700: 16.176593\n",
            "iter 1750: 16.176593\n",
            "iter 1800: 16.176593\n",
            "iter 1850: 16.176593\n",
            "iter 1900: 16.176593\n",
            "iter 1950: 16.176593\n",
            "iter 2000: 16.176593\n",
            "iter 2050: 16.176593\n",
            "iter 2100: 16.176593\n",
            "iter 2150: 16.176593\n",
            "iter 2200: 16.176593\n",
            "iter 2250: 16.176593\n",
            "iter 2300: 16.176593\n",
            "iter 2350: 16.176593\n",
            "iter 2400: 16.176593\n",
            "iter 2450: 16.176593\n",
            "iter 2500: 16.176593\n",
            "iter 2550: 16.176593\n",
            "iter 2600: 16.176593\n",
            "iter 2650: 16.176593\n",
            "iter 2700: 16.176593\n",
            "iter 2750: 16.176593\n",
            "iter 2800: 16.176593\n",
            "iter 2850: 16.176593\n",
            "iter 2900: 16.176593\n",
            "iter 2950: 16.176593\n",
            "iter 3000: 16.176593\n",
            "iter 3050: 16.176593\n",
            "iter 3100: 16.176593\n",
            "iter 3150: 16.176593\n",
            "iter 3200: 16.176593\n",
            "iter 3250: 16.176593\n",
            "iter 3300: 16.176593\n",
            "iter 3350: 16.176593\n",
            "iter 3400: 16.176593\n",
            "iter 3450: 16.176593\n",
            "iter 3500: 16.176593\n",
            "iter 3550: 16.176593\n",
            "iter 3600: 16.176593\n",
            "iter 3650: 16.176593\n",
            "iter 3700: 16.176593\n",
            "iter 3750: 16.176593\n",
            "iter 3800: 16.176593\n",
            "iter 3850: 16.176593\n",
            "iter 3900: 16.176593\n",
            "iter 3950: 16.176593\n",
            "iter 4000: 16.176593\n",
            "iter 4050: 16.176593\n",
            "iter 4100: 16.176593\n",
            "iter 4150: 16.176593\n",
            "iter 4200: 16.176593\n",
            "iter 4250: 16.176593\n",
            "iter 4300: 16.176593\n",
            "iter 4350: 16.176593\n",
            "iter 4400: 16.176593\n",
            "iter 4450: 16.176593\n",
            "iter 4500: 16.176593\n",
            "iter 4550: 16.176593\n",
            "iter 4600: 16.176593\n",
            "iter 4650: 16.176593\n",
            "iter 4700: 16.176593\n",
            "iter 4750: 16.176593\n",
            "iter 4800: 16.176593\n",
            "iter 4850: 16.176593\n",
            "iter 4900: 16.176593\n",
            "iter 4950: 16.176593\n",
            "iter 5000: 16.176593\n",
            "iter 5050: 16.176593\n",
            "iter 5100: 16.176593\n",
            "iter 5150: 16.176593\n",
            "iter 5200: 16.176593\n",
            "iter 5250: 16.176593\n",
            "iter 5300: 16.176593\n",
            "iter 5350: 16.176593\n",
            "iter 5400: 16.176593\n",
            "iter 5450: 16.176593\n",
            "iter 5500: 16.176593\n",
            "iter 5550: 16.176593\n",
            "iter 5600: 16.176593\n",
            "iter 5650: 16.176593\n",
            "iter 5700: 16.176593\n",
            "iter 5750: 16.176593\n",
            "iter 5800: 16.176593\n",
            "iter 5850: 16.176593\n",
            "iter 5900: 16.176593\n",
            "iter 5950: 16.176593\n",
            "iter 6000: 16.176593\n",
            "iter 6050: 16.176593\n",
            "iter 6100: 16.176593\n",
            "iter 6150: 16.176593\n",
            "iter 6200: 16.176593\n",
            "iter 6250: 16.176593\n",
            "iter 6300: 16.176593\n",
            "iter 6350: 16.176593\n",
            "iter 6400: 16.176593\n",
            "iter 6450: 16.176593\n",
            "iter 6500: 16.176593\n",
            "iter 6550: 16.176593\n",
            "iter 6600: 16.176593\n",
            "iter 6650: 16.176593\n",
            "iter 6700: 16.176593\n",
            "iter 6750: 16.176593\n",
            "iter 6800: 16.176593\n",
            "iter 6850: 16.176593\n",
            "iter 6900: 16.176593\n",
            "iter 6950: 16.176593\n",
            "iter 7000: 16.176593\n",
            "iter 7050: 16.176593\n",
            "iter 7100: 16.176593\n",
            "iter 7150: 16.176593\n",
            "iter 7200: 16.176593\n",
            "iter 7250: 16.176593\n",
            "iter 7300: 16.176593\n",
            "iter 7350: 16.176593\n",
            "iter 7400: 16.176593\n",
            "iter 7450: 16.176593\n",
            "iter 7500: 16.176593\n",
            "iter 7550: 16.176593\n",
            "iter 7600: 16.176593\n",
            "iter 7650: 16.176593\n",
            "iter 7700: 16.176593\n",
            "iter 7750: 16.176593\n",
            "iter 7800: 16.176593\n",
            "iter 7850: 16.176593\n",
            "iter 7900: 16.176593\n",
            "iter 7950: 16.176593\n",
            "iter 8000: 16.176593\n",
            "Saved: ridge_contour_degree_3_alpha_3.16228.png\n"
          ]
        }
      ],
      "source": [
        "# poly_gd_and_ridge_contours.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Optional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ========= CONFIG =========\n",
        "CSV_PATH = \"processed_features.csv\"   # <- change if needed\n",
        "TARGET   = \"Mu\"\n",
        "DEG_RANGE = range(0,6)               # degrees 0..5  (edit if you want 0..11)\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "# learning rates to try for stability at each degree\n",
        "LR_GRID = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4]\n",
        "# Ridge alpha search space (log grid)\n",
        "ALPHAS = np.logspace(-5, 1, 13)       # 1e-5 ... 10\n",
        "MAX_ITERS = 8000\n",
        "MAX_ITERS_HIGH_DEG = 10000\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "# ========= UTILITIES =========\n",
        "def standardize_poly_keep_bias(Xtr, Xte):\n",
        "    \"\"\"Standardize all columns except the first (bias). Ensures col0==1.\"\"\"\n",
        "    Xtr = Xtr.astype(np.float64).copy()\n",
        "    Xte = Xte.astype(np.float64).copy()\n",
        "    if Xtr.shape[1] > 1:\n",
        "        scaler = StandardScaler()\n",
        "        Xtr[:, 1:] = scaler.fit_transform(Xtr[:, 1:])\n",
        "        Xte[:, 1:] = scaler.transform(Xte[:, 1:])\n",
        "    Xtr[:, 0] = 1.0\n",
        "    Xte[:, 0] = 1.0\n",
        "    return Xtr, Xte\n",
        "\n",
        "@dataclass\n",
        "class GDResult:\n",
        "    theta: np.ndarray\n",
        "    theta_hist: np.ndarray\n",
        "    rmse_hist_train: List[float]\n",
        "\n",
        "def gd_fit(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    lr: float = 1e-3,\n",
        "    iters: int = 8000,\n",
        "    l2: float = 0.0,\n",
        "    clip: float = 1.0,\n",
        ") -> Optional[GDResult]:\n",
        "    \"\"\"\n",
        "    Batch GD minimizing MSE + l2*||theta_{1:}||^2. No regularization on bias.\n",
        "    Keeps parameter trajectory and train RMSE per iter.\n",
        "    Returns None if numerics blow up.\n",
        "    \"\"\"\n",
        "    m, d = X.shape\n",
        "    theta = np.zeros(d, dtype=np.float64)\n",
        "    theta_hist = np.empty((iters + 1, d), dtype=np.float64)\n",
        "    theta_hist[0] = theta\n",
        "    rmse_hist = []\n",
        "\n",
        "    for t in range(1, iters + 1):\n",
        "        err = X @ theta - y\n",
        "        if not np.isfinite(err).all():\n",
        "            return None\n",
        "        grad = (2.0 / m) * (X.T @ err)\n",
        "        grad[1:] += 2.0 * l2 * theta[1:]\n",
        "\n",
        "        # clip\n",
        "        gnorm = np.linalg.norm(grad)\n",
        "        if not np.isfinite(gnorm):\n",
        "            return None\n",
        "        if gnorm > clip:\n",
        "            grad = grad * (clip / gnorm)\n",
        "\n",
        "        theta -= lr * grad\n",
        "        if not np.isfinite(theta).all():\n",
        "            return None\n",
        "\n",
        "        theta_hist[t] = theta\n",
        "        rmse_hist.append(np.sqrt(mean_squared_error(y, X @ theta)))\n",
        "\n",
        "    return GDResult(theta=theta, theta_hist=theta_hist, rmse_hist_train=rmse_hist)\n",
        "\n",
        "def grid_train_gd(\n",
        "    Xtr, ytr, Xte, yte, deg: int, l2: float\n",
        ") -> Tuple[Optional[GDResult], float, float, float]:\n",
        "    iters = MAX_ITERS_HIGH_DEG if deg >= 4 else MAX_ITERS\n",
        "    for lr in LR_GRID:\n",
        "        res = gd_fit(Xtr, ytr, lr=lr, iters=iters, l2=l2, clip=GRAD_CLIP)\n",
        "        if res is None:\n",
        "            continue\n",
        "        ytr_hat = Xtr @ res.theta\n",
        "        yte_hat = Xte @ res.theta\n",
        "        tr_rmse = np.sqrt(mean_squared_error(ytr, ytr_hat))\n",
        "        te_rmse = np.sqrt(mean_squared_error(yte, yte_hat))\n",
        "        return res, tr_rmse, te_rmse, lr\n",
        "    return None, np.nan, np.nan, np.nan\n",
        "\n",
        "def make_contour(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    theta_ref: np.ndarray,\n",
        "    l2: float,\n",
        "    title: str,\n",
        "    out_path: str,\n",
        "    path_thetas: Optional[np.ndarray] = None,\n",
        "    idx0: int = 0,\n",
        "    idx1: int = 1,\n",
        "    grid_span: float = 2.5,\n",
        "    grid_pts: int = 120,\n",
        "):\n",
        "    \"\"\"\n",
        "    2D contour of objective in the plane of (theta[idx0], theta[idx1]),\n",
        "    keeping other params fixed to theta_ref.\n",
        "    \"\"\"\n",
        "    t0_c, t1_c = theta_ref[idx0], theta_ref[idx1]\n",
        "    t0s = np.linspace(t0_c - grid_span, t0_c + grid_span, grid_pts)\n",
        "    t1s = np.linspace(t1_c - grid_span, t1_c + grid_span, grid_pts)\n",
        "    Z = np.zeros((grid_pts, grid_pts), dtype=np.float64)\n",
        "\n",
        "    base = theta_ref.copy()\n",
        "    for i, v0 in enumerate(t0s):\n",
        "        for j, v1 in enumerate(t1s):\n",
        "            theta = base.copy()\n",
        "            theta[idx0] = v0\n",
        "            theta[idx1] = v1\n",
        "            resid = X @ theta - y\n",
        "            mse = np.mean(resid**2)\n",
        "            # ridge penalty excludes bias index 0\n",
        "            penalty = l2 * np.sum(theta[1:] ** 2)\n",
        "            Z[j, i] = mse + penalty  # plot objective (not RMSE) for smoother contours\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    CS = plt.contour(t0s, t1s, Z, levels=30)\n",
        "    plt.clabel(CS, inline=True, fontsize=8)\n",
        "    if path_thetas is not None:\n",
        "        plt.plot(path_thetas[:, idx0], path_thetas[:, idx1], marker=\".\", linewidth=1.5, label=\"GD path\")\n",
        "        plt.legend()\n",
        "    plt.xlabel(f\"Theta[{idx0}]\")\n",
        "    plt.ylabel(f\"Theta[{idx1}]\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# ========= MAIN WORKFLOW =========\n",
        "def main():\n",
        "    # --- load & clean ---\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        raise FileNotFoundError(f\"Could not find {CSV_PATH}. Put your CSV next to this script or update CSV_PATH.\")\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df = df.fillna(df.mean(numeric_only=True))\n",
        "\n",
        "    X = df.drop([TARGET], axis=1).to_numpy(dtype=np.float64)\n",
        "    y = df[TARGET].to_numpy(dtype=np.float64)\n",
        "    y = np.nan_to_num(y, nan=np.nanmean(y)).astype(np.float64)\n",
        "\n",
        "    # --- split & scale base features ---\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "    scaler = StandardScaler()\n",
        "    Xtr = scaler.fit_transform(Xtr).astype(np.float64)\n",
        "    Xte  = scaler.transform(Xte).astype(np.float64)\n",
        "\n",
        "    # ====== PART A: Plain GD (MSE) â€” degree selection ======\n",
        "    rows = []\n",
        "    best_deg = None\n",
        "    best_rmse = np.inf\n",
        "    best_pack = None  # (deg, res, Xtrp, Xtep, poly)\n",
        "\n",
        "    print(\"\\n---- Degree vs RMSE (Gradient Descent) ----\")\n",
        "    for deg in DEG_RANGE:\n",
        "        poly = PolynomialFeatures(degree=deg, include_bias=True)\n",
        "        Xtrp = poly.fit_transform(Xtr)\n",
        "        Xtep = poly.transform(Xte)\n",
        "        Xtrp, Xtep = standardize_poly_keep_bias(Xtrp, Xtep)\n",
        "\n",
        "        # a tiny bit more L2 for higher degrees to keep things tame\n",
        "        l2 = 1e-4 if deg <= 3 else 5e-4\n",
        "        res, tr_rmse, te_rmse, lr = grid_train_gd(Xtrp, ytr, Xtep, yte, deg, l2=l2)\n",
        "        if res is None:\n",
        "            print(f\"Degree {deg:2d} | RMSE_test = NaN (diverged)\")\n",
        "            rows.append([deg, np.nan, np.nan, np.nan, np.nan])\n",
        "            continue\n",
        "\n",
        "        print(f\"Degree {deg:2d} | RMSE_train = {tr_rmse:.6f} | RMSE_test = {te_rmse:.6f} | lr={lr:g}\")\n",
        "        rows.append([deg, tr_rmse, te_rmse, lr, l2])\n",
        "\n",
        "        if te_rmse < best_rmse:\n",
        "            best_rmse = te_rmse\n",
        "            best_deg = deg\n",
        "            best_pack = (deg, res, Xtrp, Xtep, poly, l2)\n",
        "\n",
        "    rmse_df = pd.DataFrame(rows, columns=[\"Degree\", \"Train_RMSE\", \"Test_RMSE\", \"LR_used\", \"L2_used\"])\n",
        "    rmse_df.to_csv(\"gd_degree_rmse.csv\", index=False)\n",
        "    print(\"Saved: gd_degree_rmse.csv\")\n",
        "\n",
        "    # --- Detailed outputs for the best degree (plain GD) ---\n",
        "    deg, res, Xtrp, Xtep, poly, l2 = best_pack\n",
        "    print(f\"\\nBest (plain GD) degree = {deg} with Test RMSE = {best_rmse:.6f}\")\n",
        "\n",
        "    # print RMSE every 50 iterations\n",
        "    print(\"\\nGD Train RMSE every 50 iterations (best degree):\")\n",
        "    for i in range(49, len(res.rmse_hist_train), 50):\n",
        "        print(f\"iter {i+1:4d}: {res.rmse_hist_train[i]:.6f}\")\n",
        "\n",
        "    # contour with GD path (use first two parameters)\n",
        "    make_contour(\n",
        "        Xtrp, ytr, res.theta, l2=l2,\n",
        "        title=f\"GD Objective Contour (degree={deg}) with path\",\n",
        "        out_path=f\"gd_contour_degree_{deg}.png\",\n",
        "        path_thetas=res.theta_hist\n",
        "    )\n",
        "    print(f\"Saved: gd_contour_degree_{deg}.png\")\n",
        "\n",
        "    # ====== PART B: Ridge (GD with L2) â€” degree & alpha selection ======\n",
        "    ridge_rows = []\n",
        "    ridge_best = (np.inf, None)  # (test_rmse, pack)\n",
        "    print(\"\\n---- Degree/Alpha vs RMSE (Ridge via GD) ----\")\n",
        "    for deg in DEG_RANGE:\n",
        "        poly = PolynomialFeatures(degree=deg, include_bias=True)\n",
        "        Xtrp = poly.fit_transform(Xtr)\n",
        "        Xtep = poly.transform(Xte)\n",
        "        Xtrp, Xtep = standardize_poly_keep_bias(Xtrp, Xtep)\n",
        "\n",
        "        best_alpha_for_deg = None\n",
        "        best_te_rmse_for_deg = np.inf\n",
        "        best_res_for_deg = None\n",
        "\n",
        "        for alpha in ALPHAS:\n",
        "            res, tr_rmse, te_rmse, lr = grid_train_gd(Xtrp, ytr, Xtep, yte, deg, l2=alpha)\n",
        "            if res is None:\n",
        "                continue\n",
        "            ridge_rows.append([deg, float(alpha), tr_rmse, te_rmse, lr])\n",
        "            if te_rmse < best_te_rmse_for_deg:\n",
        "                best_te_rmse_for_deg = te_rmse\n",
        "                best_alpha_for_deg = float(alpha)\n",
        "                best_res_for_deg = (res, lr)\n",
        "\n",
        "        if best_alpha_for_deg is not None:\n",
        "            print(f\"Degree {deg:2d} | best alpha={best_alpha_for_deg:g} | RMSE_test={best_te_rmse_for_deg:.6f}\")\n",
        "            if best_te_rmse_for_deg < ridge_best[0]:\n",
        "                ridge_best = (\n",
        "                    best_te_rmse_for_deg,\n",
        "                    (deg, best_alpha_for_deg, best_res_for_deg[0], Xtrp, Xtep, poly)\n",
        "                )\n",
        "        else:\n",
        "            print(f\"Degree {deg:2d} | all alphas diverged\")\n",
        "\n",
        "    ridge_df = pd.DataFrame(ridge_rows, columns=[\"Degree\", \"Alpha\", \"Train_RMSE\", \"Test_RMSE\", \"LR_used\"])\n",
        "    ridge_df.to_csv(\"ridge_degree_alpha_rmse.csv\", index=False)\n",
        "    print(\"Saved: ridge_degree_alpha_rmse.csv\")\n",
        "\n",
        "    # --- Detailed outputs for Ridge best ---\n",
        "    ridge_te_rmse, pack = ridge_best\n",
        "    rdeg, ralpha, rres, RXtrp, RXtep, Rpoly = pack\n",
        "    print(f\"\\nBest Ridge: degree={rdeg}, alpha={ralpha:g}, Test RMSE={ridge_te_rmse:.6f}\")\n",
        "\n",
        "    # print RMSE every 50 iterations (ridge)\n",
        "    print(\"\\nRidge-GD Train RMSE every 50 iterations (best degree/alpha):\")\n",
        "    for i in range(49, len(rres.rmse_hist_train), 50):\n",
        "        print(f\"iter {i+1:4d}: {rres.rmse_hist_train[i]:.6f}\")\n",
        "\n",
        "    # contour with GD path for ridge objective\n",
        "    make_contour(\n",
        "        RXtrp, ytr, rres.theta, l2=ralpha,\n",
        "        title=f\"Ridge Objective Contour (degree={rdeg}, alpha={ralpha:g}) with path\",\n",
        "        out_path=f\"ridge_contour_degree_{rdeg}_alpha_{ralpha:g}.png\",\n",
        "        path_thetas=rres.theta_hist\n",
        "    )\n",
        "    print(f\"Saved: ridge_contour_degree_{rdeg}_alpha_{ralpha:g}.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}